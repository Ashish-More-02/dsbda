{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 =\"I will walk 500 miles and I would walk 500 more .Just to be the man who walks\"+ \\\n",
    "            \"a thousand miles to fall down at your door!\"\n",
    "sentence2 =\"I played the play playfully as the players were playing in the play with playfullness\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize , sent_tokenize \n",
    "\n",
    "print ('Tokenize words :', word_tokenize(sentence1))\n",
    "print ('\\nTokenize sentence :', sent_tokenize(sentence2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
