{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample Sentences**"
      ],
      "metadata": {
        "id": "O2jMCe9hRbyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LheHRJwPFP6"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) \" + \\\n",
        "            \"and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data\"\n",
        "sentence2 = \"These insights can be used to guide decision making and strategic planning.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "2Jw25KaNRfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "print('Tokenized words:', word_tokenize(sentence1))\n",
        "print('\\nTokenized sentences:', sent_tokenize(sentence1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn08KGaRPNQj",
        "outputId": "9c320c10-c2e5-4dc7-9ac2-7be8d20d03c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['Data', 'science', 'combines', 'math', 'and', 'statistics', ',', 'specialized', 'programming', ',', 'advanced', 'analytics', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'and', 'machine', 'learning', 'with', 'specific', 'subject', 'matter', 'expertise', 'to', 'uncover', 'actionable', 'insights', 'hidden', 'in', 'an', 'organization', '’', 's', 'data']\n",
            "\n",
            "Tokenized sentences: ['Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS **Tagging**"
      ],
      "metadata": {
        "id": "QTEgDvj0Rocp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "token = word_tokenize(sentence1) + word_tokenize(sentence2)\n",
        "tagged = pos_tag(token)                 \n",
        "\n",
        "print(\"Tagging Parts of Speech:\", tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmc4AoGwPRNX",
        "outputId": "857da07e-33c7-47ba-b238-1df95eedd43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagging Parts of Speech: [('Data', 'NNP'), ('science', 'NN'), ('combines', 'NNS'), ('math', 'NN'), ('and', 'CC'), ('statistics', 'NNS'), (',', ','), ('specialized', 'VBD'), ('programming', 'NN'), (',', ','), ('advanced', 'JJ'), ('analytics', 'NNS'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('with', 'IN'), ('specific', 'JJ'), ('subject', 'JJ'), ('matter', 'NN'), ('expertise', 'NN'), ('to', 'TO'), ('uncover', 'VB'), ('actionable', 'JJ'), ('insights', 'NNS'), ('hidden', 'VBN'), ('in', 'IN'), ('an', 'DT'), ('organization', 'NN'), ('’', 'NN'), ('s', 'NN'), ('data', 'NNS'), ('These', 'DT'), ('insights', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('to', 'TO'), ('guide', 'VB'), ('decision', 'NN'), ('making', 'NN'), ('and', 'CC'), ('strategic', 'JJ'), ('planning', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop-Words Removal**"
      ],
      "metadata": {
        "id": "YJ6WRytwROE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "token = word_tokenize(sentence1)\n",
        "cleaned_token = []\n",
        "\n",
        "for word in token:\n",
        "    if word not in stop_words:\n",
        "        cleaned_token.append(word)\n",
        "\n",
        "print('Unclean version:', token)\n",
        "print('\\nCleaned version:', cleaned_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1bR4LY-PyTc",
        "outputId": "10ea1341-4205-4aca-929f-b6bedd25fab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unclean version: ['Data', 'science', 'combines', 'math', 'and', 'statistics', ',', 'specialized', 'programming', ',', 'advanced', 'analytics', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'and', 'machine', 'learning', 'with', 'specific', 'subject', 'matter', 'expertise', 'to', 'uncover', 'actionable', 'insights', 'hidden', 'in', 'an', 'organization', '’', 's', 'data']\n",
            "\n",
            "Cleaned version: ['Data', 'science', 'combines', 'math', 'statistics', ',', 'specialized', 'programming', ',', 'advanced', 'analytics', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'machine', 'learning', 'specific', 'subject', 'matter', 'expertise', 'uncover', 'actionable', 'insights', 'hidden', 'organization', '’', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# steamming "
      ],
      "metadata": {
        "id": "jM1V_VwARFIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "token = word_tokenize(sentence2)\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in token]\n",
        "print(\" \".join(stemmed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_snmLeFP6nA",
        "outputId": "805ad0c4-d61f-4f22-c10a-de7561db921a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "these insight can be use to guid decis make and strateg plan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "uRs9jXSBQzFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "token = word_tokenize(sentence2)\n",
        "\n",
        "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
        "print(\" \".join(lemmatized_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z5QV1nWQiWl",
        "outputId": "e33e0f50-720b-4eb6-f80a-c584475f034a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These insight can be used to guide decision making and strategic planning .\n"
          ]
        }
      ]
    }
  ]
}